# Project MIRAGE - Checklist

## Overview

MIRAGE advances humanâ€“AI interaction through lifelike, expressive AI avatars that listen, reason, speak, and visually react in an emotionally engaging manner.

**Use Cases:** Teachers, assistants, customer service agents, companions.

---

## Required Features

- [ ] **Multimodal Input** â€” Accept user input via text, speech, or both
- [ ] **Intelligent Response** â€” Generate responses using LLM, RAG, or rule-based approaches
- [ ] **Voice Synthesis** â€” Convert AI responses to audio via TTS or synthesized voice
- [ ] **Visual Avatar** â€” Display avatar with facial expressions and motion *(no gestures)*
- [ ] **Real-time Response** â€” Maintain smooth conversational flow without delays

---

## Bonus Features

- [ ] **Emotion Mapping** â€” Detect user sentiment and adjust avatar's tone and expressions *(facial + text emotion detection active)*
- [ ] **Personalized Memory** â€” Retain context and preferences across sessions

---

## Doable Features

> **Tech Stack:** Convai (avatar + TTS + lip-sync), React, shadcn/ui, PWA

### ðŸŸ¢ Easy

- [ ] **Interruption Response** â€” Avatar closes mouth and widens eyes when user speaks (surprise/attention)
- [ ] **Real-Time Sentiment HUD** â€” Semi-transparent display showing User Sentiment + AI State graphs
- [ ] **God Mode (Dev Toggle)** â€” Reveals live latency, raw prompts, and token usage *(pinned to right side)*

### ðŸŸ¡ Medium

- [ ] **Active Listening (Backchanneling)** â€” Avatar nods, tilts head in real-time as user speaks *(requires custom animation mapping)*
- [ ] **Storage + Context-Based Response** â€” Persist conversation history across sessions
- [ ] **Post-Interaction Report Card** â€” Summary card with topics, mood, and action items
- [ ] **Contextual Tone Shift** â€” Avatar can whisper or shout based on LLM's dramatic tags
- [ ] **The Reflection (User Analytics Dashboard)** â€” Vocabulary heatmap, sentiment trends, clarity score

### ðŸ”´ Hard

- [ ] **Volume/Pitch Mirroring** â€” TTS matches user's whisper/shout volume
- [ ] **Dynamic Environment** â€” Background changes based on conversation topic
- [ ] **Avatar Mirroring (Psychological Sync)** â€” Avatar matches user's pitch/speed for rapport
- [ ] **The Echo Gallery (Community Showcase)** â€” Grid of top interactions + shareable avatar presets

---

## Offline Fallback (PWA-Only)

> No Convai required. Fully client-side with cached models.

### Components

- [ ] **STT** â€” Vosk WASM (~50MB)
- [ ] **TTS** â€” Web Speech API / Piper WASM
- [ ] **Avatar** â€” Three.js + Ready Player Me GLB (~5-15MB)
- [ ] **LLM** â€” SmolLM / TinyLlama / Qwen via Transformers.js

### Configurations

- [ ] **Minimum Viable** â€” SmolLM-360M + Vosk + Web Speech API (~250MB)
- [ ] **Best Balance** â€” TinyLlama-1.1B + Vosk + Piper (~750MB)
- [ ] **Best Quality** â€” Llama-3.2-1B + Vosk + Piper (~850MB, WebGPU)

### Limitations (Acknowledged)

- [ ] No Convai emotion detection (build custom or skip)
- [ ] No managed avatar expressions (manual morph target control)
- [ ] WebGPU required for best LLMs
- [ ] Large initial download (cache with Service Worker)
